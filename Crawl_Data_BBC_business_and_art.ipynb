{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IRWEYjUoqKF",
        "outputId": "df991b7c-e7bc-4c15-d68f-eaa092a1b952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [61.9 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,580 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,637 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,859 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,663 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,560 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,526 kB]\n",
            "Fetched 25.8 MB in 9s (2,850 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.81.0-1ubuntu1.20).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "--2025-01-09 03:30:33--  https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.87/win64/chrome-win64.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.139.207, 74.125.141.207, 173.194.210.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.139.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 162280140 (155M) [application/x-zip-compressed]\n",
            "Saving to: ‘chrome-win64.zip’\n",
            "\n",
            "chrome-win64.zip    100%[===================>] 154.76M   200MB/s    in 0.8s    \n",
            "\n",
            "2025-01-09 03:30:34 (200 MB/s) - ‘chrome-win64.zip’ saved [162280140/162280140]\n",
            "\n",
            "--2025-01-09 03:30:34--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.141.136, 74.125.141.91, 74.125.141.190, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.141.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112846176 (108M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.62M   153MB/s    in 0.7s    \n",
            "\n",
            "2025-01-09 03:30:35 (153 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [112846176/112846176]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  mesa-vulkan-drivers\n",
            "The following NEW packages will be installed:\n",
            "  libvulkan1 mesa-vulkan-drivers\n",
            "0 upgraded, 2 newly installed, 0 to remove and 53 not upgraded.\n",
            "Need to get 10.9 MB of archives.\n",
            "After this operation, 51.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvulkan1 amd64 1.3.204.1-2 [128 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 mesa-vulkan-drivers amd64 23.2.1-1ubuntu3.1~22.04.3 [10.7 MB]\n",
            "Fetched 10.9 MB in 1s (7,909 kB/s)\n",
            "Selecting previously unselected package libvulkan1:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../libvulkan1_1.3.204.1-2_amd64.deb ...\n",
            "Unpacking libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Selecting previously unselected package mesa-vulkan-drivers:amd64.\n",
            "Preparing to unpack .../mesa-vulkan-drivers_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libvulkan1:amd64 (1.3.204.1-2) ...\n",
            "Setting up mesa-vulkan-drivers:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 123670 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (131.0.6778.264-1) ...\n",
            "Setting up google-chrome-stable (131.0.6778.264-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt repository\n",
        "!apt install -y wget curl unzip # Install utilities\n",
        "!wget https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.87/win64/chrome-win64.zip\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "\n",
        "# Fix dependencies and install Google Chrome\n",
        "!apt --fix-broken install -y\n",
        "!apt install -y libvulkan1\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp7NfCjJpDSQ",
        "outputId": "fcf17683-2963-4d5c-8b5f-cf1c3874db72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-09 03:31:07--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.141.91, 74.125.141.136, 74.125.141.190, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.141.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 112846176 (108M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb.1’\n",
            "\n",
            "google-chrome-stabl 100%[===================>] 107.62M   319MB/s    in 0.3s    \n",
            "\n",
            "2025-01-09 03:31:08 (319 MB/s) - ‘google-chrome-stable_current_amd64.deb.1’ saved [112846176/112846176]\n",
            "\n",
            "--2025-01-09 03:31:08--  https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.87/linux64/chromedriver-linux64.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.107.207, 74.125.196.207, 74.125.134.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.107.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9925370 (9.5M) [application/zip]\n",
            "Saving to: ‘chromedriver-linux64.zip’\n",
            "\n",
            "chromedriver-linux6 100%[===================>]   9.46M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-01-09 03:31:08 (106 MB/s) - ‘chromedriver-linux64.zip’ saved [9925370/9925370]\n",
            "\n",
            "Archive:  chromedriver-linux64.zip\n",
            "  inflating: chromedriver-linux64/LICENSE.chromedriver  \n",
            "  inflating: chromedriver-linux64/THIRD_PARTY_NOTICES.chromedriver  \n",
            "  inflating: chromedriver-linux64/chromedriver  \n",
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!wget https://storage.googleapis.com/chrome-for-testing-public/131.0.6778.87/linux64/chromedriver-linux64.zip\n",
        "!unzip chromedriver-linux64.zip\n",
        "!mv chromedriver-linux64/chromedriver /usr/bin/chromedriver\n",
        "!chmod +x /usr/bin/chromedriver\n",
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSqTsee9o3ib"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtSApZoepLKg"
      },
      "outputs": [],
      "source": [
        "# Configure Selenium for Headless Mode\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandbox for environments like Colab\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable shared memory usage for large pages\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration for environments with limited resources\n",
        "chrome_options.add_argument(\"--disable-software-rasterizer\")  # Further reduce memory usage\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1pHxBPHpNiB"
      },
      "outputs": [],
      "source": [
        "def get_links (url):\n",
        "  response = requests.get(url)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA-1COaTpQYJ"
      },
      "source": [
        "# Business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f52N1ajSpPVS"
      },
      "outputs": [],
      "source": [
        "# Set up Selenium WebDriver (you'll need the appropriate WebDriver installed, like ChromeDriver)\n",
        "driver.quit()\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# URL of the section you want to scrape\n",
        "url = \"https://www.bbc.com/business\"\n",
        "driver.get(url)\n",
        "\n",
        "# List to store article links\n",
        "article_links_business = set()\n",
        "titles_business = set()\n",
        "content_business = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSkJpKxRpgjS",
        "outputId": "ead35f9c-0284-46d5-a106-905ccbb25d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/news/articles/cewxvle12plo\n",
            "/news/articles/c4gzn48jwz2o\n",
            "/news/articles/crmngpvv08lo\n",
            "/news/articles/cjwlwlqpwx7o\n",
            "/news/articles/cly74mpy8klo\n",
            "/news/articles/ced835ll74jo\n",
            "/news/articles/c0j10420e2jo\n",
            "/news/articles/cwy4l3d11xro\n",
            "/news/articles/c9q78wn9g8zo\n",
            "/news/articles/c20n3x62rgjo\n",
            "/news/articles/cz6lq6x2gd9o\n",
            "/news/articles/czr3j5y0lddo\n",
            "/news/articles/c1elw2qppxvo\n",
            "/news/articles/c5yg1g7zzpgo\n",
            "/news/articles/clyg7n1d85go\n",
            "/news/articles/ce90k034mzvo\n",
            "/news/articles/ce8nlpy2n1lo\n",
            "/news/articles/cjr2wn3zvqvo\n",
            "/news/articles/c1404j3xmxdo\n",
            "/news/articles/c70ep8lp4jjo\n",
            "/news/articles/cx27zwp7jpxo\n",
            "/news/articles/cpvn9dm7yejo\n",
            "/news/articles/cgrw785585wo\n",
            "/news/articles/c0q0jl8pl9ko\n",
            "/news/articles/c23v3gg09m9o\n",
            "/news/articles/cn4xk4y2emro\n",
            "/news/articles/czxdzng92lno\n",
            "/news/articles/c205qlex5w3o\n",
            "/news/articles/c627p8leww1o\n",
            "/news/articles/c5y41qg57pno\n",
            "/news/articles/cm2v5p747xlo\n",
            "/news/articles/c623ppl5d8ro\n",
            "/news/articles/c4gj72m8x3zo\n",
            "/news/articles/cx2vz83pg9eo\n",
            "/news/articles/cx2pggj3g3po\n",
            "/news/articles/cn7r0pzz57vo\n",
            "/news/articles/cg7rl3z3jmzo\n",
            "/news/articles/cr4rvr495rgo\n",
            "/news/articles/cge93de21n0o\n",
            "/news/articles/crrwqpdv5q7o\n",
            "/news/articles/c75ngl49695o\n",
            "/news/articles/cd6084l4zx6o\n",
            "/news/articles/c4glnezg890o\n",
            "/news/articles/c4g5v8klkqvo\n"
          ]
        }
      ],
      "source": [
        "# Loop to navigate through pages and extract data\n",
        "pages = 0\n",
        "while pages != 10:\n",
        "    # Give the page time to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract article links\n",
        "    for anchor in soup.find_all(attrs={\"data-testid\": \"anchor-inner-wrapper\"}):\n",
        "        articles_link = anchor.find_all('a', attrs={\"data-testid\": \"internal-link\"}, href=True)\n",
        "        for links in articles_link:\n",
        "            if links['href'].startswith(\"/news/articles\") or links['href'].startswith(\"/future/article\") or links['href'].startswith(\"/sport/football/articles/\"):\n",
        "                article_links_business.add(links['href'])\n",
        "\n",
        "\n",
        "    # Try to find and click the \"Next\" button\n",
        "    try:\n",
        "        next_button = driver.find_element(By.XPATH, \"//button[@data-testid='pagination-next-button']\")\n",
        "        next_button.click()\n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        print(\"No more pages or 'Next' button not found\")\n",
        "        break  # Exit the loop if there's no \"Next\" button\n",
        "\n",
        "    pages += 1\n",
        "\n",
        "# Close the driver after scraping\n",
        "driver.quit()\n",
        "\n",
        "# Save the links to a file (for later use in Section 2)\n",
        "with open(\"article_links_business.txt\", \"w\") as f:\n",
        "    for link in article_links_business:\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "# Optionally, print out the links\n",
        "for link in article_links_business:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkiltSe8pprh",
        "outputId": "826c81f8-21f2-41d5-a8fd-08cfb734b2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to article_links_business.csv\n"
          ]
        }
      ],
      "source": [
        "# List to store structured data\n",
        "articles_data = []\n",
        "\n",
        "# Loop through the links\n",
        "for link in article_links_business:\n",
        "    link = link.strip()  # Remove whitespace\n",
        "    full_link = \"https://www.bbc.com\" + link\n",
        "    try:\n",
        "        # Fetch the article page\n",
        "        response = requests.get(full_link)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title found\"\n",
        "\n",
        "        # Extract and clean content\n",
        "        link_content = []\n",
        "        for contents in soup.find_all(attrs={\"data-component\": \"text-block\"}):\n",
        "            paragraph = contents.text.strip()\n",
        "            if paragraph:  # Skip empty paragraphs\n",
        "                link_content.append(paragraph)\n",
        "\n",
        "        # Combine paragraphs into a single string\n",
        "        combined_content = \" \".join(link_content) if link_content else \"No content found\"\n",
        "\n",
        "        # Append the data as a dictionary\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": title,\n",
        "            \"content\": combined_content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {full_link}: {e}\")\n",
        "        # Append error entry if needed\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": \"Error fetching title\",\n",
        "            \"content\": \"Error fetching content\"\n",
        "        })\n",
        "\n",
        "\n",
        "import csv\n",
        "# Save the data to a CSV file\n",
        "csv_file = \"article_links_business.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"title\", \"content\"])\n",
        "    writer.writeheader()  # Write the header row\n",
        "    writer.writerows(articles_data)  # Write each article as a row\n",
        "\n",
        "print(f\"Data has been saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG3sWGm7tKFE"
      },
      "source": [
        "# Executive Lounge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pgK3qDDtNdU",
        "outputId": "eabfe25e-af60-48d6-a308-6366aa9c28d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2637c220>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d852d425f8554e388d977336a5a861c\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2677e8f0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d852d425f8554e388d977336a5a861c\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2637cbb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/7d852d425f8554e388d977336a5a861c\n"
          ]
        }
      ],
      "source": [
        "driver.quit()\n",
        "\n",
        "# Configure Selenium for Headless Mode\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandbox for environments like Colab\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable shared memory usage for large pages\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration for environments with limited resources\n",
        "chrome_options.add_argument(\"--disable-software-rasterizer\")  # Further reduce memory usage\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gbFzHJmtOmP"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/business/executive-lounge\"\n",
        "driver.get(url)\n",
        "\n",
        "# List to store article links\n",
        "article_links_executive_lounge = set()\n",
        "titles_executive_lounge = set()\n",
        "content_executive_lounge = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpZqofbjth6e",
        "outputId": "7cbda0c8-fbc1-4812-9ef1-91802d4cab76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/worklife/article/20240315-lidiane-jones-bumble-ai-bbc-executive-interview\n",
            "/worklife/article/20240409-aziz-koleilat-ge-aerospace-bbc-interview\n",
            "/worklife/article/20240213-bbc-interview-melonie-parker-google\n",
            "/worklife/article/20240905-microsoft-ai-interview-bbc-executive-lounge\n",
            "/worklife/article/20240104-how-the-ethnicity-pay-gap-campaign-is-tackling-salary-inequity\n",
            "/worklife/article/20240119-how-cisco-is-bridging-the-global-digital-divide-to-connect-the-world\n",
            "/worklife/article/20240814-as-you-head-back-to-the-office-zoom-promises-its-still-relevant\n",
            "/worklife/article/20240725-legos-ceo-on-the-business-case-for-play\n",
            "/worklife/article/20240402-carla-vernon-honest-company-bbc-interview\n",
            "/worklife/article/20240320-ey-karyn-twaronite-neurodiversity-bbc-executive-interview\n",
            "/worklife/article/20240222-bbc-interview-latasha-gillespie-amazon-mgm-studios\n",
            "/worklife/article/20240710-shamina-singh-mastercard-small-business-bbc-executive-interview\n",
            "/worklife/article/20241129-how-one-product-created-a-multi-million-dollar-brand\n",
            "/worklife/article/20240307-nhung-ho-ai-intuit-bbc-executive-interview\n",
            "/worklife/article/20240206-carl-banks-giii-sports-starter-interview\n",
            "/worklife/article/20241031-what-its-like-to-work-at-the-nicest-place-on-the-internet\n",
            "/worklife/article/20240417-laura-lane-ups-bbc-executive-interview\n",
            "/worklife/article/20241004-the-simple-formula-that-made-duolingo-a-daily-habit-for-millions\n"
          ]
        }
      ],
      "source": [
        "# Loop to navigate through pages and extract data\n",
        "pages = 0\n",
        "while pages != 10:\n",
        "    # Give the page time to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract article links\n",
        "    for anchor in soup.find_all(attrs={\"data-testid\": \"anchor-inner-wrapper\"}):\n",
        "        articles_link = anchor.find_all('a', attrs={\"data-testid\": \"internal-link\"}, href=True)\n",
        "        for links in articles_link:\n",
        "            if links['href'].startswith(\"/news/articles\") or links['href'].startswith(\"/future/article\") or links['href'].startswith(\"/sport/football/articles/\") or links['href'].startswith(\"/worklife/article/\"):\n",
        "                article_links_executive_lounge.add(links['href'])\n",
        "\n",
        "\n",
        "    # Try to find and click the \"Next\" button\n",
        "    try:\n",
        "        next_button = driver.find_element(By.XPATH, \"//button[@data-testid='pagination-next-button']\")\n",
        "        next_button.click()\n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        print(\"No more pages or 'Next' button not found\")\n",
        "        break  # Exit the loop if there's no \"Next\" button\n",
        "\n",
        "    pages += 1\n",
        "\n",
        "# Close the driver after scraping\n",
        "driver.quit()\n",
        "\n",
        "# Save the links to a file (for later use in Section 2)\n",
        "with open(\"article_links_executive_lounge.txt\", \"w\") as f:\n",
        "    for link in article_links_executive_lounge:\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "# Optionally, print out the links\n",
        "for link in article_links_executive_lounge:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFWfPTYFtqU8",
        "outputId": "44091956-22c2-468e-db21-4949200639d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to article_links_executive_lounge.csv\n"
          ]
        }
      ],
      "source": [
        "# List to store structured data\n",
        "articles_data = []\n",
        "\n",
        "# Loop through the links\n",
        "for link in article_links_executive_lounge:\n",
        "    link = link.strip()  # Remove whitespace\n",
        "    full_link = \"https://www.bbc.com\" + link\n",
        "    try:\n",
        "        # Fetch the article page\n",
        "        response = requests.get(full_link)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title found\"\n",
        "\n",
        "        # Extract and clean content\n",
        "        link_content = []\n",
        "        for contents in soup.find_all(attrs={\"data-component\": \"text-block\"}):\n",
        "            paragraph = contents.text.strip()\n",
        "            if paragraph:  # Skip empty paragraphs\n",
        "                link_content.append(paragraph)\n",
        "\n",
        "        # Combine paragraphs into a single string\n",
        "        combined_content = \" \".join(link_content) if link_content else \"No content found\"\n",
        "\n",
        "        # Append the data as a dictionary\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": title,\n",
        "            \"content\": combined_content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {full_link}: {e}\")\n",
        "        # Append error entry if needed\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": \"Error fetching title\",\n",
        "            \"content\": \"Error fetching content\"\n",
        "        })\n",
        "\n",
        "\n",
        "import csv\n",
        "# Save the data to a CSV file\n",
        "csv_file = \"article_links_executive_lounge.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"title\", \"content\"])\n",
        "    writer.writeheader()  # Write the header row\n",
        "    writer.writerows(articles_data)  # Write each article as a row\n",
        "\n",
        "print(f\"Data has been saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKE1apvCuqvs"
      },
      "source": [
        "# Tech of business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs35_oT2vAwf",
        "outputId": "99dc3df1-ea66-4576-f17a-e999a914ee43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2677cdf0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d7a543e1d66fa31a741ac6e383c91039\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2677c610>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d7a543e1d66fa31a741ac6e383c91039\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d2677c280>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d7a543e1d66fa31a741ac6e383c91039\n"
          ]
        }
      ],
      "source": [
        "driver.quit()\n",
        "\n",
        "# Configure Selenium for Headless Mode\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandbox for environments like Colab\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable shared memory usage for large pages\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration for environments with limited resources\n",
        "chrome_options.add_argument(\"--disable-software-rasterizer\")  # Further reduce memory usage\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_PstoLvB4U"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/business/technology-of-business\"\n",
        "driver.get(url)\n",
        "\n",
        "# List to store article links\n",
        "article_links_ToB = set()\n",
        "titles_ToB = set()\n",
        "content_ToB = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9wtRgBFvNXg",
        "outputId": "cf2e8568-b29c-4c54-83d4-ac123d0036fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/news/articles/cgej7gzg8l0o\n",
            "/news/articles/c80e54yjzdmo\n",
            "/news/articles/c4gv0rvx0dvo\n",
            "/news/articles/cqllvze3204o\n",
            "/future/article/20240911-the-scientists-and-chasers-seeking-insights-inside-the-eye-of-the-most-powerful-storms\n",
            "/news/articles/c8j7md2kj8vo\n",
            "/news/articles/cy430qpez81o\n",
            "/news/articles/c511p8gdn21o\n",
            "/news/articles/ckgxv7jk0z1o\n",
            "/news/articles/cj9n1939ryzo\n",
            "/news/articles/cmllxl89jlwo\n",
            "/news/articles/cd114lllyp6o\n",
            "/news/articles/c4nn6z6z4ypo\n",
            "/news/articles/ce817156p34o\n",
            "/news/articles/c33em6jrx1go\n",
            "/news/articles/cyx6xv95n23o\n",
            "/news/articles/cwy3qp15x73o\n",
            "/news/articles/c62g03ln7z4o\n",
            "/news/articles/ce58nvdg327o\n",
            "/news/articles/clyjvn658n6o\n",
            "/news/articles/c0kk4lxe702o\n",
            "/news/articles/cn47zg3xgxxo\n",
            "/news/articles/c722xdddl4po\n",
            "/worklife/article/20240130-the-gaming-industry-is-aiming-for-subscribers-will-gamers-play-along\n",
            "/news/articles/cy9j10wj78wo\n",
            "/news/articles/c1m0ylerjevo\n",
            "/news/articles/crg4d5vn0n0o\n",
            "/news/articles/c93pz1dz2kxo\n",
            "/news/articles/clmydee2grno\n",
            "/news/articles/cqxwjwqw3xdo\n",
            "/news/articles/c254ggrry45o\n",
            "/news/articles/c4gvnym0j0xo\n",
            "/news/articles/c99z7wwq1lgo\n",
            "/news/articles/cml2pyvmw9ro\n",
            "/news/articles/cd1646ewzleo\n",
            "/news/articles/c288m1km01po\n",
            "/news/articles/cd116m3jyn1o\n",
            "/news/articles/cx7dx48ev91o\n",
            "/news/articles/crk4pm8xxx8o\n",
            "/news/articles/ce9zx22ley8o\n",
            "/news/articles/cx99qv2w1ddo\n",
            "/worklife/article/20240126-tech-start-ups-are-giving-a-slick-makeover-to-us-healthcare-services\n",
            "/news/articles/c51yd4j4lnvo\n",
            "/worklife/article/20240325-artificial-intelligence-ai-us-agriculture-farming\n",
            "/news/articles/cqxj700pxpdo\n",
            "/news/articles/c4gp555xy5ro\n",
            "/news/articles/cg64pwxzln4o\n",
            "/news/articles/cj5ll89dy2mo\n",
            "/news/articles/c870j92p82wo\n",
            "/news/articles/c8el64yyppro\n",
            "/news/articles/cl44mv0jnv5o\n",
            "/news/articles/cq8v1ww51vno\n",
            "/news/articles/cq8v73dkne3o\n",
            "/news/articles/cg4lvw6vzkyo\n",
            "/news/articles/c24pvm8ly18o\n",
            "/news/articles/clkkllxm4jzo\n",
            "/news/articles/crrldyvqe4xo\n",
            "/news/articles/crrwqpdv5q7o\n",
            "/news/articles/c74l9p2x3dxo\n",
            "/news/articles/c8vzzjqyz5qo\n",
            "/worklife/article/20240223-ai-could-make-the-four-day-workweek-inevitable\n",
            "/news/articles/c1e8q4j1yygo\n",
            "/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination\n",
            "/news/articles/c6ppjq0zz1go\n",
            "/news/articles/c3g3413p1gxo\n",
            "/news/articles/crge77e9j19o\n",
            "/news/articles/c4g5v8klkqvo\n"
          ]
        }
      ],
      "source": [
        "# Loop to navigate through pages and extract data\n",
        "pages = 0\n",
        "while pages != 13:\n",
        "    # Give the page time to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract article links\n",
        "    for anchor in soup.find_all(attrs={\"data-testid\": \"anchor-inner-wrapper\"}):\n",
        "        articles_link = anchor.find_all('a', attrs={\"data-testid\": \"internal-link\"}, href=True)\n",
        "        for links in articles_link:\n",
        "            if links['href'].startswith(\"/news/articles\") or links['href'].startswith(\"/future/article\") or links['href'].startswith(\"/sport/football/articles/\") or links['href'].startswith(\"/worklife/article\"):\n",
        "                article_links_ToB.add(links['href'])\n",
        "\n",
        "\n",
        "    # Try to find and click the \"Next\" button\n",
        "    try:\n",
        "        next_button = driver.find_element(By.XPATH, \"//button[@data-testid='pagination-next-button']\")\n",
        "        next_button.click()\n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        print(\"No more pages or 'Next' button not found\")\n",
        "        break  # Exit the loop if there's no \"Next\" button\n",
        "\n",
        "    pages += 1\n",
        "\n",
        "# Close the driver after scraping\n",
        "driver.quit()\n",
        "\n",
        "# Save the links to a file (for later use in Section 2)\n",
        "with open(\"article_links_ToB.txt\", \"w\") as f:\n",
        "    for link in article_links_ToB:\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "# Optionally, print out the links\n",
        "for link in article_links_ToB:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy6c3XxF305T",
        "outputId": "7883303b-3e2e-4dbe-dca5-cd6e183615b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to article_links_ToB.csv\n"
          ]
        }
      ],
      "source": [
        "# List to store structured data\n",
        "articles_data = []\n",
        "\n",
        "# Loop through the links\n",
        "for link in article_links_ToB:\n",
        "    link = link.strip()  # Remove whitespace\n",
        "    full_link = \"https://www.bbc.com\" + link\n",
        "    try:\n",
        "        # Fetch the article page\n",
        "        response = requests.get(full_link)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title found\"\n",
        "\n",
        "        # Extract and clean content\n",
        "        link_content = []\n",
        "        for contents in soup.find_all(attrs={\"data-component\": \"text-block\"}):\n",
        "            paragraph = contents.text.strip()\n",
        "            if paragraph:  # Skip empty paragraphs\n",
        "                link_content.append(paragraph)\n",
        "\n",
        "        # Combine paragraphs into a single string\n",
        "        combined_content = \" \".join(link_content) if link_content else \"No content found\"\n",
        "\n",
        "        # Append the data as a dictionary\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": title,\n",
        "            \"content\": combined_content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {full_link}: {e}\")\n",
        "        # Append error entry if needed\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": \"Error fetching title\",\n",
        "            \"content\": \"Error fetching content\"\n",
        "        })\n",
        "\n",
        "\n",
        "import csv\n",
        "# Save the data to a CSV file\n",
        "csv_file = \"article_links_ToB.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"title\", \"content\"])\n",
        "    writer.writeheader()  # Write the header row\n",
        "    writer.writerows(articles_data)  # Write each article as a row\n",
        "\n",
        "print(f\"Data has been saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcKeHHJF5G5q"
      },
      "source": [
        "# Future of Business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTUm_PnL55iv",
        "outputId": "200ec6c4-3ba5-49f9-f41d-c9b492b47976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d264adfc0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d9749a48710e659e52290ffea6a8c65c\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d264aee30>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d9749a48710e659e52290ffea6a8c65c\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d264aded0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/d9749a48710e659e52290ffea6a8c65c\n"
          ]
        }
      ],
      "source": [
        "driver.quit()\n",
        "\n",
        "# Configure Selenium for Headless Mode\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandbox for environments like Colab\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable shared memory usage for large pages\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration for environments with limited resources\n",
        "chrome_options.add_argument(\"--disable-software-rasterizer\")  # Further reduce memory usage\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svOYmxxQ5_py"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/business/future-of-business\"\n",
        "driver.get(url)\n",
        "\n",
        "# List to store article links\n",
        "article_links_FoB = set()\n",
        "titles_FoB = set()\n",
        "content_FoB = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcpXfuPF59In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d895f8-393f-4a48-e640-0209a0375bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/news/articles/cgej7gzg8l0o\n",
            "/future/article/20241125-the-sinking-island-states-fighting-climate-change\n",
            "/news/articles/c78dwepw95do\n",
            "/future/article/20241002-the-us-race-to-release-electric-pickup-trucks\n",
            "/future/article/20240911-the-scientists-and-chasers-seeking-insights-inside-the-eye-of-the-most-powerful-storms\n",
            "/future/article/20241028-the-failure-that-started-the-internet\n",
            "/news/articles/c7912nx2n5lo\n",
            "/future/article/20241121-tuvalu-the-pacific-islands-creating-a-digital-nation-in-the-metaverse-due-to-climate-change\n",
            "/future/article/20240923-100-year-old-cat-memes-that-predate-the-internet\n",
            "/news/articles/c6248j078x1o\n",
            "/future/article/20241001-how-satellites-are-mapping-the-future-of-turtle-conservation\n",
            "/news/articles/c1m0ylerjevo\n",
            "/news/articles/c5yd18q248jo\n",
            "/future/article/20241219-the-oregon-trail-how-a-50-year-old-video-game-defined-america\n",
            "/future/article/20240426-the-ghosts-of-indias-tiktok-social-media-ban\n",
            "/news/articles/cqxj700pxpdo\n",
            "/news/articles/c870j92p82wo\n",
            "/news/articles/cq8v73dkne3o\n",
            "/future/article/20241115-how-robotaxis-are-trying-to-win-passengers-trust\n",
            "/news/articles/cze3p1j710ko\n",
            "/news/articles/crrldyvqe4xo\n",
            "/news/articles/cz04e232dk1o\n",
            "/news/articles/cd0elzk24dno\n",
            "/news/articles/c4g5v8klkqvo\n",
            "/worklife/article/20241004-the-simple-formula-that-made-duolingo-a-daily-habit-for-millions\n",
            "/news/articles/cqjz04lv5q9o\n",
            "/future/article/20241129-drilling-the-deepest-holes-on-earth-how-to-bore-12-miles-into-our-planets-crust\n",
            "/news/articles/cqj0wkwv1x2o\n",
            "/news/articles/cj9n1939ryzo\n",
            "/news/articles/cdxvnwkl5kgo\n",
            "/news/articles/c33em6jrx1go\n",
            "/future/article/20240930-how-technology-creates-hidden-work-for-women\n",
            "/future/article/20241008-the-troubling-future-of-ai-relationships\n",
            "/news/articles/clyjvn658n6o\n",
            "/future/article/20241025-the-smartwatches-that-can-predict-parkinsons-disease\n",
            "/future/article/20241030-the-weird-way-ai-assistants-get-their-names\n",
            "/news/articles/cqxwjwqw3xdo\n",
            "/news/articles/cm2evjxev4jo\n",
            "/news/articles/cy9eegg0rdvo\n",
            "/news/articles/cwy3dz0614jo\n",
            "/news/articles/cyvym6gyvd7o\n",
            "/future/article/20240912-what-riddles-teach-us-about-the-human-mind\n",
            "/news/articles/c4gp555xy5ro\n",
            "/news/articles/cqj051glrr9o\n",
            "/news/articles/cq8v1ww51vno\n",
            "/news/articles/cvg3442p966o\n",
            "/news/articles/cge93de21n0o\n",
            "/worklife/article/20241129-how-one-product-created-a-multi-million-dollar-brand\n",
            "/news/articles/c89v1w5lxxqo\n",
            "/news/articles/c8j943l7lk4o\n",
            "/news/articles/c3weg0qzq4zo\n",
            "/news/articles/cy430qpez81o\n",
            "/news/articles/c079zp2vy31o\n",
            "/news/articles/cwy3qp15x73o\n",
            "/news/articles/c0lwgn9p8z4o\n",
            "/news/articles/c20e3l3xllwo\n",
            "/news/articles/cy9j10wj78wo\n",
            "/future/article/20240923-the-feud-at-the-beginning-of-new-yorks-electricity\n",
            "/news/articles/ce9zx22ley8o\n",
            "/news/articles/cze386d3enpo\n",
            "/news/articles/cwyl171lyewo\n",
            "/news/articles/cjr431lr72jo\n",
            "/news/articles/cd0zedgdvdlo\n",
            "/news/articles/c1wjwdwjdxdo\n",
            "/news/articles/c8el64yyppro\n",
            "/news/articles/c77jx4d5748o\n",
            "/news/articles/cn7r8xr3v76o\n",
            "/future/article/20250107-the-key-climate-and-nature-moments-to-look-out-for-in-2025\n",
            "/news/articles/cg4lvw6vzkyo\n",
            "/news/articles/c24pvm8ly18o\n",
            "/news/articles/c89xvjkzzyvo\n",
            "/news/articles/crrwqpdv5q7o\n",
            "/news/articles/c2dl70wed1lo\n",
            "/future/article/20241018-ai-art-the-end-of-creativity-or-a-new-movement\n",
            "/news/articles/cdd4e2931q3o\n",
            "/future/article/20241220-an-ai-started-tasting-colours-and-shapes-that-is-more-human-than-you-might-think\n",
            "/news/articles/c80e54yjzdmo\n",
            "/future/article/20241217-the-controversial-machine-using-marine-carbon-removal-to-store-co2-in-the-ocean\n",
            "/news/articles/c8j7md2kj8vo\n",
            "/news/articles/ckgxv7jk0z1o\n",
            "/news/articles/ce817156p34o\n",
            "/news/articles/c4gzz5wdg41o\n",
            "/news/articles/c62g03ln7z4o\n",
            "/news/articles/c8dmp1jg448o\n",
            "/news/articles/cjdn10yk0k1o\n",
            "/news/articles/cz6lq6x2gd9o\n",
            "/news/articles/c93pz1dz2kxo\n",
            "/news/articles/c4gvnym0j0xo\n",
            "/future/article/20241031-how-google-tells-you-what-you-want-to-hear\n",
            "/news/articles/c0qz3dg21vqo\n",
            "/news/articles/crk4pm8xxx8o\n",
            "/news/articles/cwyxd1p98yro\n",
            "/future/article/20241018-barcodes-at-75-how-black-and-white-lines-went-into-space-and-stoked-fears-of-the-antichrist\n",
            "/news/articles/cg64pwxzln4o\n",
            "/news/articles/czr7kggvzmmo\n",
            "/future/article/20241101-how-online-photos-and-videos-alter-the-way-you-think\n",
            "/future/article/20240927-how-coal-fired-power-stations-are-being-turned-into-batteries\n",
            "/future/article/20240822-why-we-need-a-better-way-to-measure-hurricanes\n",
            "/future/article/20241122-ai-deepfakes-is-there-something-special-about-the-human-voice\n",
            "/news/articles/c1e8q4j1yygo\n",
            "/future/article/20241120-chinas-ant-forest-users-cheat-in-game-that-helps-the-climate\n",
            "/news/articles/c5ywp0l0xrxo\n",
            "/future/article/20241210-how-stumbleupon-pioneered-the-way-we-use-the-internet\n"
          ]
        }
      ],
      "source": [
        "# Loop to navigate through pages and extract data\n",
        "pages = 0\n",
        "while pages != 13:\n",
        "    # Give the page time to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract article links\n",
        "    for anchor in soup.find_all(attrs={\"data-testid\": \"anchor-inner-wrapper\"}):\n",
        "        articles_link = anchor.find_all('a', attrs={\"data-testid\": \"internal-link\"}, href=True)\n",
        "        for links in articles_link:\n",
        "            if links['href'].startswith(\"/news/articles\") or links['href'].startswith(\"/future/article\") or links['href'].startswith(\"/sport/football/articles/\") or links['href'].startswith(\"/worklife/article\"):\n",
        "                article_links_FoB.add(links['href'])\n",
        "\n",
        "\n",
        "    # Try to find and click the \"Next\" button\n",
        "    try:\n",
        "        next_button = driver.find_element(By.XPATH, \"//button[@data-testid='pagination-next-button']\")\n",
        "        next_button.click()\n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        print(\"No more pages or 'Next' button not found\")\n",
        "        break  # Exit the loop if there's no \"Next\" button\n",
        "\n",
        "    pages += 1\n",
        "\n",
        "# Close the driver after scraping\n",
        "driver.quit()\n",
        "\n",
        "# Save the links to a file (for later use in Section 2)\n",
        "with open(\"article_links_FoB.txt\", \"w\") as f:\n",
        "    for link in article_links_FoB:\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "# Optionally, print out the links\n",
        "for link in article_links_FoB:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1D5riB06clw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790e00be-14b1-4630-80ff-e0c0afdc38a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to article_links_FoB.csv\n"
          ]
        }
      ],
      "source": [
        "# List to store structured data\n",
        "articles_data = []\n",
        "\n",
        "# Loop through the links\n",
        "for link in article_links_FoB:\n",
        "    link = link.strip()  # Remove whitespace\n",
        "    full_link = \"https://www.bbc.com\" + link\n",
        "    try:\n",
        "        # Fetch the article page\n",
        "        response = requests.get(full_link)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title found\"\n",
        "\n",
        "        # Extract and clean content\n",
        "        link_content = []\n",
        "        for contents in soup.find_all(attrs={\"data-component\": \"text-block\"}):\n",
        "            paragraph = contents.text.strip()\n",
        "            if paragraph:  # Skip empty paragraphs\n",
        "                link_content.append(paragraph)\n",
        "\n",
        "        # Combine paragraphs into a single string\n",
        "        combined_content = \" \".join(link_content) if link_content else \"No content found\"\n",
        "\n",
        "        # Append the data as a dictionary\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": title,\n",
        "            \"content\": combined_content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {full_link}: {e}\")\n",
        "        # Append error entry if needed\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": \"Error fetching title\",\n",
        "            \"content\": \"Error fetching content\"\n",
        "        })\n",
        "\n",
        "\n",
        "import csv\n",
        "# Save the data to a CSV file\n",
        "csv_file = \"article_links_FoB.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"title\", \"content\"])\n",
        "    writer.writeheader()  # Write the header row\n",
        "    writer.writerows(articles_data)  # Write each article as a row\n",
        "\n",
        "print(f\"Data has been saved to {csv_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W7f4xe483fx"
      },
      "source": [
        "# Arts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C8xSJ8D9HE7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00a7bc1-6723-4d2a-bd52-e58fc97d4204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d098b0a00>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/ce02d5b0c9d8695ddb0c3f32dd25b15b\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d098b24d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/ce02d5b0c9d8695ddb0c3f32dd25b15b\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x786d098b2050>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/ce02d5b0c9d8695ddb0c3f32dd25b15b\n"
          ]
        }
      ],
      "source": [
        "driver.quit()\n",
        "\n",
        "# Configure Selenium for Headless Mode\n",
        "# Set up Chrome options to run in headless mode\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without GUI)\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Disable sandbox for environments like Colab\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Disable shared memory usage for large pages\n",
        "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration for environments with limited resources\n",
        "chrome_options.add_argument(\"--disable-software-rasterizer\")  # Further reduce memory usage\n",
        "\n",
        "# Set up Selenium WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuNrGHnD-D-m"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.bbc.com/arts\"\n",
        "driver.get(url)\n",
        "\n",
        "# List to store article links\n",
        "article_links_art = set()\n",
        "titles_art = set()\n",
        "content_art = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qISCoJBJ-D_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2fd5e0-d0e8-4152-83a4-4afd46fbceba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/news/articles/cx2nprmpr20o\n",
            "/news/articles/cre72vw5x3do\n",
            "/news/articles/cdxz6gekq9do\n",
            "/news/articles/cy9qz8qrgrlo\n",
            "/news/articles/cwyx0340p8yo\n",
            "/news/articles/cyv3j07dz15o\n",
            "/news/articles/cy8y97x8xm0o\n",
            "/news/articles/cx2650n7qrxo\n",
            "/news/articles/cqx8we9n8dqo\n",
            "/news/articles/c8xjv021y07o\n",
            "/news/articles/cly24zpnnnko\n",
            "/news/articles/c05p3jv4r54o\n",
            "/news/articles/cvgp45pp33lo\n",
            "/news/articles/cr56mr5r7e2o\n",
            "/news/articles/c8rj550x7pyo\n",
            "/news/articles/cgrwv8gz449o\n",
            "/news/articles/cn0xr33dj4po\n",
            "/news/articles/cvgn14n64gqo\n",
            "/news/articles/cx2px80z5k0o\n",
            "/news/articles/c2kxndpy5klo\n",
            "/news/articles/c62wngxd6x1o\n",
            "/news/articles/cx2y5zv77e8o\n",
            "/news/articles/c4gp0evepd6o\n",
            "/news/articles/c70e9z72d5vo\n",
            "/news/articles/cn85xpvv5v8o\n",
            "/news/articles/ced896gjg1po\n",
            "/news/articles/c07g1zn5k8mo\n",
            "/news/articles/cp83n2mvme8o\n",
            "/news/articles/ce8x5066m5vo\n",
            "/news/articles/clyx887e35zo\n",
            "/news/articles/cgl9007jwgdo\n",
            "/news/articles/c3dxez91gy7o\n",
            "/news/articles/cy0nq011j7ko\n",
            "/news/articles/cvgx1756eyvo\n",
            "/news/articles/c0474pd93wyo\n",
            "/news/articles/cr7v9xd7zvko\n",
            "/news/articles/ced8wv7qn3no\n",
            "/news/articles/cn85vy57l2xo\n",
            "/news/articles/c99xvn427nlo\n",
            "/news/articles/ckg90evm28vo\n",
            "/news/articles/cn42ewgjz17o\n",
            "/news/articles/cdr0j7njkj0o\n",
            "/news/articles/c74xl77mlwpo\n",
            "/news/articles/c7913qjn2v1o\n",
            "/news/articles/c704drqg1yko\n",
            "/news/articles/c5yd49w4zw0o\n",
            "/news/articles/c7458yjglzpo\n",
            "/news/articles/c5y7qjqyx9vo\n",
            "/news/articles/cj4vpd1yrdvo\n",
            "/news/articles/c8dq95d29jyo\n",
            "/news/articles/c206yx90k40o\n",
            "/news/articles/cn4xleywgpjo\n",
            "/news/articles/c1d3r0p7017o\n",
            "/news/articles/c9831rr772lo\n",
            "/news/articles/cr4rdykp9wwo\n",
            "/news/articles/c0lg6151p32o\n",
            "/news/articles/c4gl1n0gepqo\n",
            "/news/articles/c0ewgjpn2z5o\n",
            "/news/articles/cx2pp82pn9wo\n",
            "/news/articles/c390ve7d11lo\n",
            "/news/articles/c789pev9dgpo\n",
            "/news/articles/cwyxkm2xpgeo\n",
            "/news/articles/c623q4jr87lo\n",
            "/news/articles/c04lweek16zo\n",
            "/news/articles/cd603wpq1vyo\n",
            "/news/articles/c0lgk8pkj6lo\n",
            "/news/articles/c0ewgvrrgn7o\n",
            "/news/articles/c86w1l18v1yo\n",
            "/news/articles/c75wyx1ly0po\n",
            "/news/articles/cewx2w72k79o\n",
            "/news/articles/cwy31nv72e7o\n",
            "/news/articles/c3e3982dz8ko\n",
            "/news/articles/cd0e2d5z0m9o\n",
            "/news/articles/cewxdx7j00ro\n",
            "/news/articles/c1mrdkekdx1o\n",
            "/news/articles/c5y81392jg7o\n",
            "/news/articles/cly4q9d5vx8o\n",
            "/news/articles/cly2vjdpdgyo\n",
            "/news/articles/c86qzy6vx2xo\n",
            "/news/articles/cz7qvle5yd8o\n",
            "/news/articles/cn7rv58ljpgo\n",
            "/news/articles/cewxv1rg4lxo\n",
            "/news/articles/cy4pgx5z9l4o\n",
            "/news/articles/cj6z3pn25r0o\n",
            "/news/articles/c93g7413y96o\n",
            "/news/articles/cy53y11d252o\n",
            "/news/articles/c4gp69965xjo\n",
            "/news/articles/cqjzyjrdknlo\n",
            "/news/articles/cly75pm447do\n",
            "/news/articles/c627357pqexo\n",
            "/news/articles/c20ell72x1qo\n",
            "/news/articles/c3dxkd22mnjo\n",
            "/news/articles/cy7kl54rlyxo\n",
            "/news/articles/cql5ewy5ypvo\n",
            "/news/articles/c62wprwv6dvo\n"
          ]
        }
      ],
      "source": [
        "# Loop to navigate through pages and extract data\n",
        "pages = 0\n",
        "while pages != 13:\n",
        "    # Give the page time to load\n",
        "    time.sleep(3)\n",
        "\n",
        "    # Parse the page using BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Extract article links\n",
        "    for anchor in soup.find_all(attrs={\"data-testid\": \"anchor-inner-wrapper\"}):\n",
        "        articles_link = anchor.find_all('a', attrs={\"data-testid\": \"internal-link\"}, href=True)\n",
        "        for links in articles_link:\n",
        "            if links['href'].startswith(\"/news/articles\") or links['href'].startswith(\"/future/article\") or links['href'].startswith(\"/sport/football/articles/\") or links['href'].startswith(\"/worklife/article\"):\n",
        "                article_links_art.add(links['href'])\n",
        "\n",
        "\n",
        "    # Try to find and click the \"Next\" button\n",
        "    try:\n",
        "        next_button = driver.find_element(By.XPATH, \"//button[@data-testid='pagination-next-button']\")\n",
        "        next_button.click()\n",
        "        time.sleep(3)\n",
        "    except:\n",
        "        print(\"No more pages or 'Next' button not found\")\n",
        "        break  # Exit the loop if there's no \"Next\" button\n",
        "\n",
        "    pages += 1\n",
        "\n",
        "# Close the driver after scraping\n",
        "driver.quit()\n",
        "\n",
        "# Save the links to a file (for later use in Section 2)\n",
        "with open(\"article_links_art.txt\", \"w\") as f:\n",
        "    for link in article_links_art:\n",
        "        f.write(link + \"\\n\")\n",
        "\n",
        "# Optionally, print out the links\n",
        "for link in article_links_art:\n",
        "    print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZrWZTF1_F4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d24a858-e3d9-4834-9772-15f9a6a5bf07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to article_links_art.csv\n"
          ]
        }
      ],
      "source": [
        "# List to store structured data\n",
        "articles_data = []\n",
        "\n",
        "# Loop through the links\n",
        "for link in article_links_art:\n",
        "    link = link.strip()  # Remove whitespace\n",
        "    full_link = \"https://www.bbc.com\" + link\n",
        "    try:\n",
        "        # Fetch the article page\n",
        "        response = requests.get(full_link)\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract the title\n",
        "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title found\"\n",
        "\n",
        "        # Extract and clean content\n",
        "        link_content = []\n",
        "        for contents in soup.find_all(attrs={\"data-component\": \"text-block\"}):\n",
        "            paragraph = contents.text.strip()\n",
        "            if paragraph:  # Skip empty paragraphs\n",
        "                link_content.append(paragraph)\n",
        "\n",
        "        # Combine paragraphs into a single string\n",
        "        combined_content = \" \".join(link_content) if link_content else \"No content found\"\n",
        "\n",
        "        # Append the data as a dictionary\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": title,\n",
        "            \"content\": combined_content\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {full_link}: {e}\")\n",
        "        # Append error entry if needed\n",
        "        articles_data.append({\n",
        "            \"link\": full_link,\n",
        "            \"title\": \"Error fetching title\",\n",
        "            \"content\": \"Error fetching content\"\n",
        "        })\n",
        "\n",
        "\n",
        "import csv\n",
        "# Save the data to a CSV file\n",
        "csv_file = \"article_links_art.csv\"\n",
        "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"link\", \"title\", \"content\"])\n",
        "    writer.writeheader()  # Write the header row\n",
        "    writer.writerows(articles_data)  # Write each article as a row\n",
        "\n",
        "print(f\"Data has been saved to {csv_file}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}